1. In the introduction, four example applications are mentioned for which the dataset can be used. According to the title of the paper the dataset is proposed for phenotyping applications. Thus, the authors should elaborate on why and how the computer vision tasks they mention are relevant to plant phenotyping. The authors should also include few key references to works that use (or discuss) such features for plant phenotyping.

We added the following paragraph in the Introduction section to better connect plant phenotyping and computer vision tasks. We added the related references as well.

Plants develop through a complex interaction between genotype and environment. This determines their structure and functions and thus performance such as yield or efficient use of resources. In order to understand the genetic basis of these economically important parameters, it is essential to quantitatively assess plant phenotypes and then identify the latent relationships to genotypes and environmental factors. Plant visual phenotyping has been performed by farmers and breeders for more than 5,000 years. In the past, traditional phenotyping is based on experience and intuition, and is laborious~\cite{Erblichkeit1903}. Recent progresses in imaging sensor, robotics and automation technologies lead to the development of the ever-increasing new field of highly automated, non-destructive {\it plant visual phenotyping}~\cite{furbank2011phenomics,cruz2015depi}. The objective of modern  plant visual phenotyping is to analyze and categorize the morphological  characteristics of plants, thus accurately quantifying plant traits. %It advances crop yield improvement and enables the systematic study of the environmental plasticity of plants. In this interdisciplinary field, scientists employ various imaging sensors to capture plants and design advanced algorithms to automatically analyze the captured plant imagery, with the purpose of raising testable biological hypotheses to solve problems related to growth, development, stress tolerance, resistance, and so on. A key advance in visual phenotyping is the capability to non-invasively capture plant traits, enabling continuous measurements that are necessary to monitor plants during growth and/or under stress conditions. Vision based phenotyping also increases the throughput of phenotyping experiments by eliminating destructive operations. The increased capacity allows more genotypes or biological replicates to be examined under the same environmental conditions~\cite{fahlgren2015lights,walter2015plant}.

2. Since multi-modality is a strength of the proposed dataset, the authors should motivate better their choice of imaging modalities (i.e. fluorescence, infrared, RGB color, and depth), highlighting their importance and what information they convey on plant structure and functions. Apparently, this is only partly done for depth at the end of Sec. 2.

We added the following paragraph in the end of Section 2. 

The four sensing modalities provide unique opportunities to comprehensively characterize plant morphological and physiological phenotypes. Chlorophyll fluorescence measurement is an effective method to detect photosynthesis. For example, steady-state chlorophyll fluorescence is a relative measure of photosynthetic health in drought stress~\cite{chen2014dissecting}. The quantum efficiency of photosystem II can be approximated by comparing chlorophyll fluorescence levels under two light conditions~\cite{baker2008chlorophyll}. The dense depth measurement has been a component of a number of recent non-plant RGB-D databases designed for object recognition~\cite{Lai2011}, scene segmentation~\cite{Silberman2011}, human analysis~\cite{Sung2011,Barbosa:reid12}, and mapping~\cite{sturm12iros}. By including dense depth for a plant database we anticipate enabling development of new 3D plant canopy analysis algorithms, thus probing the total energy intake and storage. The infrared measurement enables researchers to detect chloroplast movements, which have been proposed to optimize light capture and minimize photodamage to the photosynthetic apparatus~\cite{Dutta2015Non}.  

3. It appears from Sec. 3.1 that all plant subjects (respectively for Arabidopsis and bean) belong to the same genotype and that no treatments were performed. This would entail that the proposed dataset cannot be used to investigate computer vision algorithms or imaging modalities in relation to group differences. Could the authors comment on this?
[JEFF]

4. In Sec. 3.2.1 the authors remark that light used for night image acquisition does not influence plant development. A reference should be included to support this statement. As for the other lighting and imaging protocols adopted by the authors (Sec. 3.2.1 and 3.2.2), do they interfere with plant growth?

We revised the following paragraph and added two references.

Since $940~nm$ light does not influence plant development or drive photosynthesis, images were also collected during the night period~\cite{eskins1992light}. Precisely, phytochromes are the closest photoreceptors that absorb in the far red, near infrared region, but the action spectrum is diminished to zero or near zero ($>800~nm$) in the region where we do IR reflectance ($940~nm$)~\cite{butler1964actton}. Note that the other modalities were captured only at day time, so that they will not interfere plant growth.

5. Furthermore, it assumes that cameras are perfectly synchronized which i) is not mentioned and ii) it cannot be done for some modalities since the same camera (with different filter) is used so some delay is expected. Granted the plants may not move in between but this should be clarified and mentioned.


6. On table 3 you list resolutions for the acquired image data. This image resolution is much lower than the LSC database of Scharr et al. This limitation should be mentioned.

7) Figure 2 if possible can you please also include an image showing the cameras from the view of the plant (LED off)?

