\section{Annotation, Files and Protocol}
\subsection{Data Statistics}
MSU-PID includes two subsets, one for each plant type: Arabidopsis and bean.
The statistic information of these two subsets are summarized in Table~\ref{tab:stat}.
The images were acquired every hour.
As there is no light at night hours, plants can not be imaged by the fluorescence and RGB color sensors while IR and depth cameras can still perform the capturing during the night.
In order to make sure that all four modalities are present at each imaging modality, we release the part of images captured only in the day time, which are $15$ images per day for Arabidopsis and $14$ for bean for all four modalities.

The two subsets differ in plant image resolutions.
As shown in Fig.~\ref{fig:fourmodality}, we grow and image a single bean plant while a whole tray of Arabidopsis are grown at the same time.
Therefore, the resolution of a Arabidopsis plant is much lower than that of a bean plant.
We manually crop $16$ Arabidopsis plants, which have been captured by all four sensors simultaneously.
Table~\ref{tab:resolution} summaries the image resolution of each plant in all four modalities.


\begin{figure*}
\centering
\includegraphics[width=.90\textwidth]{Figures/labeling}\\
\caption{Leaf labeling process, including tip labels and leaf segmentation annotation.}
\label{fig:label}
\end{figure*}

%\begin{figure*}
%\begin{centering}
%\begin{tabular}{@{}c@{} c@{} c@{} c@{} c@{} c@{} c@{} c@{} c@{}}
%%\begin{tabular}{lllllllll}
%\includegraphics[width=.11\textwidth]{Figures/labelExample/plant_1_day_1_num_13.png}&
%\includegraphics[width=.11\textwidth]{Figures/labelExample/plant_1_day_2_num_13.png}&
%\includegraphics[width=.11\textwidth]{Figures/labelExample/plant_1_day_3_num_13.png}&
%\includegraphics[width=.11\textwidth]{Figures/labelExample/plant_1_day_4_num_13.png}&
%\includegraphics[width=.11\textwidth]{Figures/labelExample/plant_1_day_5_num_13.png}&
%\includegraphics[width=.11\textwidth]{Figures/labelExample/plant_1_day_6_num_13.png}&
%\includegraphics[width=.11\textwidth]{Figures/labelExample/plant_1_day_7_num_13.png}&
%\includegraphics[width=.11\textwidth]{Figures/labelExample/plant_1_day_8_num_13.png}&
%\includegraphics[width=.11\textwidth]{Figures/labelExample/plant_1_day_9_num_13.png}\\
%day 1 & day 2 & day 3 & day 4 & day 5 & day 6 & day 7 & day 8 & day 9 \\
%\end{tabular}
%\caption{Leaf labeling results of one Arabidopsis plant over nine days with one image per day. Note that the labels of tips are not shown for visualization clarity. }
%\label{fig:LabelExample}
%\end{centering}
%\end{figure*}

It is often argued which is the best setting due to the trade-off between the image resolution and the throughput of phenotyping experiments.
We chose to image a whole tray of Arabidopsis at lower resolution rather than an individual at higher resolution, because it better reflects high-throughput protocols, which in practice allows direct comparison of multiple genotypes in the same experiment.
In contrary, we chose to image a single bean plant because we anticipate enabling development of $3$D plant canopy reconstruction algorithms for bush plants.


\subsection{Manual Annotations}
Part of the database is manually annotated to provide ground truth tip locations, leaf segmentation results, and leaf consistency overtime.
Tip locations are saved in a TXT file for each frame.
Leaf segmentation results are stored in a PNG image for each frame with one color for each leaf.
The same color is used to represent the same leaf over a sequence of frames.

We use the fluorescence images as the input for labeling because of their simple and uniform background.
For Arabidopsis images, we label $4$ frames each day.
While for bean images, we label $7$ frames each day because of their spontaneous and faster leaf movement.
A Matlab GUI interface is developed for leaf labeling, as shown in Fig.~\ref{fig:label}, which will also be available to the public.
A user can open a plant image to label the two tips and annotate each leaf segment.
The results will be automatically saved once a user moves onto the next image for labeling.
For consistent annotation of the same leaf over time, we show a number on the center of each leaf indicating the order of labeling from the previous frame.

The labeling of leaf tips is implemented by clicking pairs of points on the image.
The outer tip is always clicked first before the inner tip.
For visualization, a line connecting each pair of tips will be shown immediately after clicking the inner tip.
Inaccurate labels can be deleted by clicking the right button of the mouse near the labeled point and relabeled by clicking the left button again immediately after deletion.
All tips can be deleted by clicking "Clear Tip" and relabel again. 

The labeling of the leaf segment is implemented by clicking the boundary of one leaf at each time.
In order to provide more accuracy labeling, we click very dense points ($\sim20$ points on average) on the boundary.
The labeled leaf boundary is overlaid on the image for better visualization to guide the next action.
Incorrect label can be deleted right after the labeling by clicking ``Clear".
All leaf labels can be deleted by clicking ``Clear All".
This process continues until all leaf segments have been annotated.
Once a leaf is invisible due to occlusion, the label can be skipped by clicking ``Invisible". 
After the labeling of one plant, we visually go through the results and correct inaccurate labels.
One example of the labeling results for one plant is shown in Fig.~\ref{fig:trackExample} $(b)$, where one color is used to represent each specific leaf.
As we can see during the transition between day $5$ and day $6$, there is one leaf showing up and covering up the leaf underneath, which disappears and will not be annotated later.
In total, we labeled $5,142$ leaves.

Note that one alternative approach for labeling leaf segments is to directly label the membership of superpixels instead of drawing a polygon along the boundary.
Our experience is that since a noticeable percentage of extracted superpixels cover pixels of two neighboring leaves, the extra effort of breaking a super pixel into two makes it a less efficient alternative.


\subsection{Label Propagation}
\textcolor{red}{input by Dr. Morris on how the transformation is done}

\begin{figure*}
\begin{centering}
\begin{tabular}{cccc}
\includegraphics[width=.18\textwidth]{Figures/LabelAlignment/day_3_hour_23-seg_ir.png}&
\includegraphics[width=.18\textwidth]{Figures/LabelAlignment/day_3_hour_23-seg_fmp.png}&
\includegraphics[width=.18\textwidth]{Figures/LabelAlignment/day_3_hour_23-seg_rgb.png}&
\includegraphics[width=.18\textwidth]{Figures/LabelAlignment/day_3_hour_23-seg_depth.png}\\
\includegraphics[width=.18\textwidth]{Figures/LabelAlignment/day_5_hour_23-seg_ir.png}&
\includegraphics[width=.18\textwidth]{Figures/LabelAlignment/day_5_hour_23-seg_fmp.png}&
\includegraphics[width=.18\textwidth]{Figures/LabelAlignment/day_5_hour_23-seg_rgb.png}&
\includegraphics[width=.18\textwidth]{Figures/LabelAlignment/day_5_hour_23-seg_depth.png}\\
\includegraphics[width=.18\textwidth]{Figures/LabelAlignment/day_9_hour_20-seg_ir.png}&
\includegraphics[width=.18\textwidth]{Figures/LabelAlignment/day_9_hour_20-seg_fmp.png}&
\includegraphics[width=.18\textwidth]{Figures/LabelAlignment/day_9_hour_20-seg_rgb.png}&
\includegraphics[width=.18\textwidth]{Figures/LabelAlignment/day_9_hour_20-seg_depth.png}\\
($a$) & ($b$) & ($c$) & ($d$) \\
\end{tabular}
\caption{Label propagation between all four modalities ($a$) infra-red, ($b$) fluorescence, ($c$) color and ($d$) depth, of a sample plant in the Arabidopsis collection for day 3 (top row), day 5 (middle row), and day 9 (bottom row).  In the dataset the manual segmentation is performed on the fluorescence images and is outlined here as orange lines propagated to all modalities.  The infra-red image is taken by the same camera and so will have close to exact pixel correspondence.  To assess the segmented pixel propagation to other modalities, we also manually labeled a subset of the color plant images and propagated this segmentation to all modalities (shown in cyan).  Comparing these boundaries in each modality gives a measure of the propagation errors due to parallax, and we provide quantitative analysis in table XXX.  Note that the color and depth images are rotated 90 degrees.}
\label{fig:LabelAlignment}
\end{centering}
\end{figure*}


%In order to evaluate the performance of label propagation, we use one of our evaluation metric Symmetric Best Dice (SBD), as will describe in Sec.~\ref{sec:protocol}, which is used to evaluate the similarity between two segment masks. 
%Table~\ref{tab:labelError} shows the performance of label propagation vs. the performance of human labels.
%We compare the label difference between two annotators $L_1$ and $L_2$.


\begin{table*}[t]
\centering
\caption{Human label performance vs. label propagation performance.}
\begin{tabular}{c|c|c|c|c|c|c}
\hline
    & \multicolumn{3}{c|}{fmp modality} & \multicolumn{3}{c}{RGB modality}\\ \hline
    & $L_1$ vs. $L_1$ & $L_1$ vs. $L_2$  & $L_1$ vs. $L_t$ & $L_1$ vs. $L_1$ & $L_1$ vs. $L_2$ & $L_1$ vs. $L_t$ \\ \hline
   plant$\_3\_$day$\_3\_$hour$\_23$ & $1$ & $0.750$ & $0.798$ & $1$ & $0.827$ & $0.800$ \\ \hline
   plant$\_3\_$day$\_5\_$hour$\_23$ & $1$ & $0.793$ & $0.846$ & $1$ & $0.871$ & $0.847$ \\ \hline
   plant$\_3\_$day$\_9\_$hour$\_20$ & $1$ & $0.837$ & $0.787$ & $1$ & $0.877$ & $0.789$ \\ \hline
                   average &          $1$ & $0.793$ & $0.810$ & $1$ & $0.858$ & $0.812$ \\ \hline
\end{tabular}
\label{tab:labelError}
\end{table*}




\subsection{Name Conventions and File Types}
We release training and testing sets in two separate folders.
In each folder, there are two subfolders named Arabidopsis and Bean.
The files in each subfolder have the following form:

\begin{itemize}
%\item plant$\_$ID$\_$day$\_$X$\_$hour$\_$YY$\_$rgb.png: the original RGB color images;
%\item plant$\_$ID$\_$day$\_$X$\_$hour$\_$YY$\_$fmp.png: the original fluorescence images;
%\item plant$\_$ID$\_$day$\_$X$\_$hour$\_$YY$\_$ir.png: the original IR images;
%\item plant$\_$ID$\_$day$\_$X$\_$hour$\_$YY$\_$depth.png: the original depth images;
\item plant$\_$ID$\_$day$\_$X$\_$hour$\_$YY$\_$modality.png: the original images in each modality separately;
%\item plant$\_$ID$\_$day$\_$X$\_$hour$\_$YY$\_$label.png: the labeled images of fluorescence modality;
%\item plant$\_$ID$\_$day$\_$X$\_$hour$\_$YY$\_$tips.txt: the labeled tip locations;
\item plant$\_$ID$\_$day$\_$X$\_$hour$\_$YY$\_$label$\_$modality.png: the labeled images of each modality if available;
\item plant$\_$ID$\_$day$\_$X$\_$hour$\_$YY$\_$tips$\_$modality.txt: the labeled tip locations of each modality if available;
\item plant$\_$ID$\_$day$\_$X$\_$hour$\_$YY$\_$depthSigma.png: the depth standard deviation images;
\end{itemize}
where ID indicates the plant subject ID number ($1$ to $16$ for Arabidopsis, $1$ to $5$ for bean), X is an integer indicating the date ($1-9$ for Arabidopsis, $1$ to $5$ for bean), and YY represents the image index within a day ($1-15$ for Arabidopsis, $1$ to $14$ for bean).
For each combination of day and hour, we provide four modalities in PNG files ($\_$rgb, $\_$fmp, $\_$ir, $\_$depth).
%\begin{itemize}
% \item plant$\_$XX$\_$day$\_$X$\_$hour$\_$YY$\_$ID$\_$ZZ$\_$rgb.png: the original RGB color images;
% \item plant$\_$XX$\_$day$\_$X$\_$hour$\_$YY$\_$ID$\_$ZZ$\_$fmp.png: the original fluorescence images;
% \item plant$\_$XX$\_$day$\_$X$\_$hour$\_$YY$\_$ID$\_$ZZ$\_$ir.png: the original IR images;
% \item plant$\_$XX$\_$day$\_$X$\_$hour$\_$YY$\_$ID$\_$ZZ$\_$depth.png: the original depth images;
% \item plant$\_$XX$\_$day$\_$X$\_$hour$\_$YY$\_$ID$\_$ZZ$\_$label.png: the labeled images of fluorescence modality;
% \item plant$\_$XX$\_$day$\_$X$\_$hour$\_$YY$\_$ID$\_$ZZ$\_$tips.txt: the labeled tip locations;
%\end{itemize}
%where XX indicates the plant type ("AR" or "BE"), X is an integer indicating the date (e.g., $1-9$ for Arabidopsis), YY represents the image index within a day (e.g., $1-16$ for Arabidopsis), and ZZ is the subject (or plant) ID (e.g., $1-5$ for bean).
%For each combination of day and hour, we provide four modalities in PNG files ($\_$rgb, $\_$fmp, $\_$ir, $\_$depth).
For annotated images, we have two additional files ($\_$label, $\_$tips) saving the annotation results.
Leaf segmentation results are encoded as indexed PNG files, where each leaf is assigned a unique and consistent leaf ID over time.
Leaf ID starts from $1$ and continuously increase till the total number of leaves.
And the background is encoded as $0$.
Tips locations are saved in TXT files where each line has the following format:
\begin{itemize}
\item leaf ID \quad tip1$\_$x \quad tip1$\_$y \quad tip2$\_$x \quad tip2$\_$y
\end{itemize}
where leaf ID is an integer number that is consistent with the segmentation label in the PNG file.
tip1$\_$x and tip1$\_$y represent the coordinates of the outer tip point.
tip2$\_$x and tip2$\_$y represent the coordinates of the inner tip point.


%In addition to the original images and annotation results, we provide another folder named Matlab with all Matlab functions that will be used for mapping between different image modalities and for the purpose of performance evaluation.
%Note that the annotation is provided based on fluorescence images.
%In order to evaluate methods developed on other modalities, we provide image-mapping functions between every two modalities.
The total storage of our database is around $380 MB$, which is convenient for downloading via Internet.

\begin{figure*}[t!]
\centering
\includegraphics[width=.98\textwidth]{Figures/overview}\\
\caption{Overview of the baseline method.}
\label{fig:methodOverview}
\end{figure*}

\subsection{Experimental Protocols}
\label{sec:protocol}
As shown in Table~\ref{tab:database}, MSU-PID can be used for applications such as leaf segmentation, leaf alignment, leaf tracking, and leaf counting.
To facilitate future research, we separate the database into the training set and the testing set.
$40\%$ of the data is used for training and $60\%$ for testing.
Specifically, $6$ plants of Arabidopsis and $2$ plants of bean are selected for training.
%We will provide training and testing data in different folders.
For fair comparison, both supervised learning and unsupervised learning methods should evaluate their performance on the training and testing sets separately.
The user may decide to utilize one or multiple modalities of the plant imagery for training and testing respectively.
The availability of multiple modalities allows user to design novel experimental setup.
For example, using RGB and depth modalities for training and RGB for testing can take advantage of additional information during the learning without incurring extra sensor cost during the testing, which can be implemented via either learning with side information~\cite{chen2013boosting}, or transfering learning with missing modality~\cite{ding2014latent}.
%The evaluation metrics will be discussed in Sec.~\ref{sec:baseline}.


\paragraph{Performance Metric}
To evaluate the performance of leaf segmentation, alignment, tracking, and counting, we use four performance metrics, whose Matlab implementations will be provided along with the data.
Three of them are based on the tip-based error, which is defined as the average distance of a pair of estimated leaf tips $\hat{\bm{t}}_{1,2}$ with a pair of labeled leaf tips $ \bm{t}_{1,2}$ normalized by the labeled leaf length:
\begin {equation}
e_{la}(\hat{\bm{t}}_{1,2}, \bm{t}_{1,2}) = \frac{||\hat{\bm{t}}_1-{\bm{t}}_1||_2 + ||\hat{\bm{t}}_2-{\bm{t}}_2||_2}{2 ||\bm{t}_1-\bm{t}_2||_2}.
\label{eqn:tipError}
\end{equation}

We build the frame-to-frame and video-to-video correspondence respectively and generate two sets of tip-based errors.
More details can be find in~\cite{yin2015}.
We define a threshold $\tau$ to operate on the corresponding tip-based errors.
By varying $\tau$, we compute the first three metrics as follows:
\begin{itemize}
\item {\it{Unmatched Leaf Rate (ULR)}}, the percentage of unmatched leaves with respect to the total number of labeled leaves.  
This can attribute to two sources.
The first is miss detections and false alarms.
The second is matched leaves with tip-based errors larger than $\tau$.
When $\tau$ is large enough, this value is equal to the leaf counting error.
\item {\it{Landmark Error (LE)}}, the average tip-based errors smaller than $\tau$ of all frame-to-frame correspondent leaves.
This is used to measure the leaf tip alignment error. 
\item {\it{Tracking Consistency (TC)}}, the percentage of video-to-video correspondent leaves whose tip-based errors are smaller than $\tau$.
This is used to measure leaf tracking error. 
\end{itemize}

In order to evaluate the leaf segmentation accuracy, we adopt an additional metric~\cite{scharr2014annotated} based on the Dice score of estimated segmentation results and ground truth labels:
\begin{itemize}
\item {\it{Symmetric Best Dice (SBD)}}, the symmetric best Dice among all labeled leaves.
\end{itemize}
The Matlab function for computing {\it{SBD}} is provided by~\cite{scharr2014annotated}.
%The instructions on how to use the evaluation functions are included as comments of the function.


%For leaf annotation, we use the idea of merging and splitting super pixels.
%There are six different numbers of super pixels: $100$, $200$, $300$, $500$, $800$, $1000$.
%The user can specify which level to use depending on how well the super pixels can separate the leaves from the background.
%Because one leaf can be covered by several super pixels and one super pixel can across two leaves or the foreground and background.
%We allow merging and splitting super pixels.
%Merging is implemented by drawing a rectangle on the image.
%Every super pixel overlapping with this rectangle will be merged together.
%Splitting is implemented by drawing a line separating a leaf from the background or from another leaf.
%
%As shown in Fig.~\ref{fig:label}, several super pixels that covers one leaf are first merged together to form a large super pixel.
%Since the top part of the super pixel covers some of the background and the bottom part covers another small leaf, two lines are drawn on this super pixel to split it.
%The leaf boundary is overlaid on the image for better visualization to guide the next action.
%This process continues until all leaves have been annotated.




